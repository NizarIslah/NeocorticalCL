{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43895ec1-52a7-43d4-acfb-360cc24c0ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nislah/.vmgr_repo/dev-2021-02-py38/lib/python3.8/site-packages/avalanche/training/plugins/evaluation.py:84: UserWarning: No benchmark provided to the evaluation plugin. Metrics may be computed on inconsistent portion of streams, use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting experiment...\n",
      "Start training on experience  0\n",
      "End training on experience  0\n",
      "Computing accuracy on the test set\n",
      "dict_iCaRL_aia=  {'Top1_Acc_Stream/Exp0': 0.923}\n",
      "scifar100-batch=10 Average Incremental Accuracy: 0.92300\n",
      "Start training on experience  1\n",
      "End training on experience  1\n",
      "Computing accuracy on the test set\n",
      "dict_iCaRL_aia=  {'Top1_Acc_Stream/Exp0': 0.923, 'Top1_Acc_Stream/Exp1': 0.85}\n",
      "scifar100-batch=10 Average Incremental Accuracy: 0.88650\n",
      "Start training on experience  2\n",
      "End training on experience  2\n",
      "Computing accuracy on the test set\n",
      "dict_iCaRL_aia=  {'Top1_Acc_Stream/Exp0': 0.923, 'Top1_Acc_Stream/Exp1': 0.85, 'Top1_Acc_Stream/Exp2': 0.7843333333333333}\n",
      "scifar100-batch=10 Average Incremental Accuracy: 0.85244\n",
      "Start training on experience  3\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP, IncrementalClassifier\n",
    "from avalanche.training.strategies import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC, CoPE\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.training.strategies import BaseStrategy\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, GEMPlugin, GDumbPlugin\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR10, SplitCIFAR100\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics,ExperienceForgetting\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.strategies import Naive\n",
    "from pl_bolts.models.self_supervised import SwAV\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import SGD\n",
    "from torchvision import transforms\n",
    "from avalanche.training.strategies.icarl import ICaRL\n",
    "import numpy as np\n",
    "from avalanche.benchmarks.classic.ccifar100 import SplitCIFAR100\n",
    "from avalanche.models import IcarlNet, make_icarl_net, initialize_icarl_net\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "\n",
    "from NeoCL.models.pretrained import PretrainedIncrementalClassifier, SSLIcarl\n",
    "from NeoCL.plugins.sparse_ewc import SparseEWCPlugin\n",
    "from NeoCL.strategies.utils import get_average_metric, create_default_args\n",
    "\n",
    "\n",
    "# create strategy\n",
    "def icarl_cifar100_augment_data(img):\n",
    "    img = img.numpy()\n",
    "    padded = np.pad(img, ((0, 0), (4, 4), (4, 4)), mode='constant')\n",
    "    random_cropped = np.zeros(img.shape, dtype=np.float32)\n",
    "    crop = np.random.randint(0, high=8 + 1, size=(2,))\n",
    "\n",
    "    # Cropping and possible flipping\n",
    "    if np.random.randint(2) > 0:\n",
    "        random_cropped[:, :, :] = \\\n",
    "            padded[:, crop[0]:(crop[0]+32), crop[1]:(crop[1]+32)]\n",
    "    else:\n",
    "        random_cropped[:, :, :] = \\\n",
    "            padded[:, crop[0]:(crop[0]+32), crop[1]:(crop[1]+32)][:, :, ::-1]\n",
    "    t = torch.tensor(random_cropped)\n",
    "    return t\n",
    "fixed_class_order = [87, 0, 52, 58, 44, 91, 68, 97, 51, 15,\n",
    "                            94, 92, 10, 72, 49, 78, 61, 14, 8, 86,\n",
    "                            84, 96, 18, 24, 32, 45, 88, 11, 4, 67,\n",
    "                            69, 66, 77, 47, 79, 93, 29, 50, 57, 83,\n",
    "                            17, 81, 41, 12, 37, 59, 25, 20, 80, 73,\n",
    "                            1, 28, 6, 46, 62, 82, 53, 9, 31, 75,\n",
    "                            38, 63, 33, 74, 27, 22, 36, 3, 16, 21,\n",
    "                            60, 19, 70, 90, 89, 43, 5, 42, 65, 76,\n",
    "                            40, 30, 23, 85, 2, 95, 56, 48, 71, 64,\n",
    "                            98, 13, 99, 7, 34, 55, 54, 26, 35, 39]\n",
    "# config (NOTE: memory_size==k)\n",
    "args = create_default_args({'cuda': 0, 'batch_size': 128, 'nb_exp': 10,\n",
    "                            'memory_size': 2000, 'epochs': 70, 'lr_base': 2.,\n",
    "                            'lr_milestones': [49, 63], 'lr_factor': 5.,\n",
    "                            'wght_decay': 0.00001, 'train_mb_size': 256,\n",
    "                            'fixed_class_order': fixed_class_order, 'seed': 2222})\n",
    "#\n",
    "device = torch.device(f\"cuda:{args.cuda}\"\n",
    "                      if torch.cuda.is_available() and\n",
    "                         args.cuda >= 0 else \"cpu\")\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
    "encoder = SwAV.load_from_checkpoint(weight_path, strict=True)\n",
    "model = SSLIcarl(encoder,embedding_size=2048,num_classes=100).to(device)\n",
    "tb_logger = TensorboardLogger('../logs')\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    ExperienceForgetting(),\n",
    "    loggers=[tb_logger])\n",
    "benchmark = SplitCIFAR100(n_experiences=args.nb_exp, seed=args.seed,\n",
    "              fixed_class_order=args.fixed_class_order, dataset_root='/share/datasets/')\n",
    "\n",
    "\n",
    "optim = SGD(model.parameters(), lr=args.lr_base,\n",
    "            weight_decay=args.wght_decay, momentum=0.9)\n",
    "sched = LRSchedulerPlugin(\n",
    "    MultiStepLR(optim, args.lr_milestones, gamma=1.0 / args.lr_factor))\n",
    "\n",
    "strategy = ICaRL(\n",
    "    model.feature_extractor, model.classifier, optim,\n",
    "    args.memory_size,\n",
    "    buffer_transform=transforms.Compose([icarl_cifar100_augment_data]),\n",
    "    fixed_memory=True, train_mb_size=args.batch_size,\n",
    "    train_epochs=args.epochs, eval_mb_size=args.batch_size,\n",
    "    plugins=[sched], device=device, evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "# train on the selected scenario with the chosen strategy\n",
    "print('Starting experiment...')\n",
    "dict_iCaRL_aia = {}\n",
    "for i, train_batch_info in enumerate(benchmark.train_stream):\n",
    "    print(\"Start training on experience \", train_batch_info.current_experience)\n",
    "\n",
    "    strategy.train(train_batch_info, num_workers=4)\n",
    "    print(\"End training on experience \", train_batch_info.current_experience)\n",
    "    print('Computing accuracy on the test set')\n",
    "    res = strategy.eval(benchmark.test_stream[:i + 1], num_workers=4)\n",
    "    dict_iCaRL_aia['Top1_Acc_Stream/Exp'+str(i)] = res['Top1_Acc_Stream/eval_phase/test_stream/Task000']\n",
    "    avg_ia = get_average_metric(dict_iCaRL_aia)\n",
    "    print(\"dict_iCaRL_aia= \", dict_iCaRL_aia)\n",
    "    print(f\"scifar100-batch=10 Average Incremental Accuracy: {avg_ia:.5f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c0bf3-ef70-4cb4-bae6-d426617926a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "       # Dict to iCaRL Evaluation Protocol: Average Incremental Accuracy\n",
    "        dict_iCaRL_aia = {}\n",
    "        # ___________________________________________train and eval\n",
    "        for i, exp in enumerate(benchmark.train_stream):\n",
    "            strategy.train(exp, num_workers=4)\n",
    "            res = strategy.eval(benchmark.test_stream[:i + 1], num_workers=4)\n",
    "            dict_iCaRL_aia['Top1_Acc_Stream/Exp'+str(i)] = res['Top1_Acc_Stream/eval_phase/test_stream/Task000']\n",
    "\n",
    "        avg_ia = get_average_metric(dict_iCaRL_aia)\n",
    "        target_acc = get_target_result('iCaRL', 'scifar100')\n",
    "        print(\"dict_iCaRL_aia= \", dict_iCaRL_aia)\n",
    "        print(f\"scifar100-batch=10 Average Incremental Accuracy: {avg_ia:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452945-47b4-432b-ac1d-ff61e6c556ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dev-2021-02-py38': venv)",
   "language": "python",
   "name": "python38564bitdev202102py38venved5311bfb7744d6caaf9ece0ad44c9d0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
