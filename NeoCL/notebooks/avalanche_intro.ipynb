{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe36c03-4bfa-412f-81e8-75d9f47cc7a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nislah/.vmgr_repo/dev-2021-02-py38/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "Resizing mnist ([28, 28]->[32, 32])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing mnist ([28, 28]->[32, 32])\n",
      "  0%|                                 | 0/182040794 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to /home/nislah/.ctrl_data/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182041600it [00:47, 3801999.34it/s]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to /home/nislah/.ctrl_data/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64275456it [00:18, 3526214.27it/s]                                  \n",
      "  0%|                                 | 0/170498071 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/nislah/.ctrl_data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:43, 3892220.21it/s]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nislah/.ctrl_data/cifar-10-python.tar.gz to /home/nislah/.ctrl_data\n",
      "Files already downloaded and verified\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26422272it [00:07, 3732524.60it/s]                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nislah/.ctrl_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 288483.84it/s]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nislah/.ctrl_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422656it [00:01, 2924630.12it/s]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nislah/.ctrl_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [00:00, 22704672.93it/s]                                     \n",
      "Resizing fashion-mnist ([28, 28]->[32, 32])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nislah/.ctrl_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/nislah/.ctrl_data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing fashion-mnist ([28, 28]->[32, 32])\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading DTD dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625246208it [02:49, 3681834.86it/s]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 2/2 [00:16<00:00,  8.40s/it]\n",
      "  0%|                                         | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to /home/nislah/.ctrl_data/dtd/processed/dtd-train-32x32.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 1/1 [00:07<00:00,  7.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to /home/nislah/.ctrl_data/dtd/processed/dtd-test-32x32.th\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /home/nislah/.ctrl_data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169001984it [01:01, 2736135.14it/s]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nislah/.ctrl_data/cifar-100-python.tar.gz to /home/nislah/.ctrl_data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ctrl\n",
    "benchmark = ctrl.get_stream('s_pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a0142-34b2-4f00-867d-1ad297f29d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([400, 3, 32, 32]), torch.Size([400]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = next(iter(benchmark))\n",
    "x,y = task.get_data(split=0'=, task.get_labels(split=00)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c183c5-e8f4-4d06-b907-759df7807d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting experiment...\n",
      "Start training on experience  0\n",
      "End training on experience  0\n",
      "Computing accuracy on the test set\n",
      "dict_iCaRL_aia=  {'Top1_Acc_Stream/Exp0': 0.909}\n",
      "scifar100-batch=10 Average Incremental Accuracy: 0.90900\n",
      "Start training on experience  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP, IncrementalClassifier\n",
    "from avalanche.training.strategies import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC, CoPE\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.training.strategies import BaseStrategy\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, GEMPlugin, GDumbPlugin\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR10, SplitCIFAR100\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics,ExperienceForgetting\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.strategies import Naive\n",
    "from pl_bolts.models.self_supervised import SwAV\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import SGD\n",
    "from torchvision import transforms\n",
    "from avalanche.training.strategies.icarl import ICaRL\n",
    "import numpy as np\n",
    "from avalanche.benchmarks.classic.ccifar100 import SplitCIFAR100\n",
    "from avalanche.models import IcarlNet, make_icarl_net, initialize_icarl_net\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "\n",
    "from NeoCL.models.pretrained import PretrainedIncrementalClassifier, SSLIcarl\n",
    "from NeoCL.plugins.sparse_ewc import SparseEWCPlugin\n",
    "from NeoCL.strategies.utils import get_average_metric, create_default_args\n",
    "from torch import nn\n",
    "import ctrl\n",
    "\n",
    "# create strategy\n",
    "def icarl_cifar100_augment_data(img):\n",
    "    img = img.numpy()\n",
    "    padded = np.pad(img, ((0, 0), (4, 4), (4, 4)), mode='constant')\n",
    "    random_cropped = np.zeros(img.shape, dtype=np.float32)\n",
    "    crop = np.random.randint(0, high=8 + 1, size=(2,))\n",
    "\n",
    "    # Cropping and possible flipping\n",
    "    if np.random.randint(2) > 0:\n",
    "        random_cropped[:, :, :] = \\\n",
    "            padded[:, crop[0]:(crop[0]+32), crop[1]:(crop[1]+32)]\n",
    "    else:\n",
    "        random_cropped[:, :, :] = \\\n",
    "            padded[:, crop[0]:(crop[0]+32), crop[1]:(crop[1]+32)][:, :, ::-1]\n",
    "    t = torch.tensor(random_cropped)\n",
    "    return t\n",
    "fixed_class_order = [87, 0, 52, 58, 44, 91, 68, 97, 51, 15,\n",
    "                            94, 92, 10, 72, 49, 78, 61, 14, 8, 86,\n",
    "                            84, 96, 18, 24, 32, 45, 88, 11, 4, 67,\n",
    "                            69, 66, 77, 47, 79, 93, 29, 50, 57, 83,\n",
    "                            17, 81, 41, 12, 37, 59, 25, 20, 80, 73,\n",
    "                            1, 28, 6, 46, 62, 82, 53, 9, 31, 75,\n",
    "                            38, 63, 33, 74, 27, 22, 36, 3, 16, 21,\n",
    "                            60, 19, 70, 90, 89, 43, 5, 42, 65, 76,\n",
    "                            40, 30, 23, 85, 2, 95, 56, 48, 71, 64,\n",
    "                            98, 13, 99, 7, 34, 55, 54, 26, 35, 39]\n",
    "# config (NOTE: memory_size==k)\n",
    "seed = np.random.randint(1000, 9999)\n",
    "args = create_default_args({'cuda': 0, 'batch_size': 128, 'nb_exp': 10,\n",
    "                            'memory_size': 2000, 'epochs': 70, 'lr_base': 2.,\n",
    "                            'lr_milestones': [49, 63], 'lr_factor': 5.,\n",
    "                            'wght_decay': 0.00001, 'train_mb_size': 256,\n",
    "                            'fixed_class_order': fixed_class_order, 'seed': seed})\n",
    "#\n",
    "device = torch.device(f\"cuda:{args.cuda}\"\n",
    "                      if torch.cuda.is_available() and\n",
    "                         args.cuda >= 0 else \"cpu\")\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
    "encoder = SwAV.load_from_checkpoint(weight_path, strict=True)\n",
    "model = SSLIcarl(encoder,embedding_size=2048,num_classes=100).to(device)\n",
    "tb_logger = TensorboardLogger(f'../logs/{args.seed}/')\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    ExperienceForgetting(),\n",
    "    loggers=[tb_logger])\n",
    "benchmark = SplitCIFAR100(n_experiences=args.nb_exp, seed=args.seed,\n",
    "              fixed_class_order=args.fixed_class_order, dataset_root='/share/datasets/')\n",
    "\n",
    "criterion=CrossEntropyLoss()\n",
    "optim = SGD(model.parameters(), lr=args.lr_base,\n",
    "            weight_decay=args.wght_decay, momentum=0.9)\n",
    "sched = LRSchedulerPlugin(\n",
    "    MultiStepLR(optim, args.lr_milestones, gamma=1.0 / args.lr_factor))\n",
    "\n",
    "strategy = ICaRL(\n",
    "    model.feature_extractor, model.classifier, optim,\n",
    "    args.memory_size, buffer_transform=transforms.Compose([icarl_cifar100_augment_data]),\n",
    "    train_mb_size=args.batch_size, fixed_memory=True,\n",
    "    train_epochs=args.epochs, eval_mb_size=args.batch_size,\n",
    "    plugins=[sched], device=device, evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "# train on the selected scenario with the chosen strategy\n",
    "print('Starting experiment...')\n",
    "dict_iCaRL_aia = {}\n",
    "for i, train_batch_info in enumerate(benchmark.train_stream):\n",
    "    print(\"Start training on experience \", train_batch_info.current_experience)\n",
    "\n",
    "    strategy.train(train_batch_info, num_workers=4)\n",
    "    print(\"End training on experience \", train_batch_info.current_experience)\n",
    "    print('Computing accuracy on the test set')\n",
    "    res = strategy.eval(benchmark.test_stream[:i + 1], num_workers=4)\n",
    "    dict_iCaRL_aia['Top1_Acc_Stream/Exp'+str(i)] = res['Top1_Acc_Stream/eval_phase/test_stream/Task000']\n",
    "    avg_ia = get_average_metric(dict_iCaRL_aia)\n",
    "    print(\"dict_iCaRL_aia= \", dict_iCaRL_aia)\n",
    "    print(f\"scifar100-batch=10 Average Incremental Accuracy: {avg_ia:.5f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43895ec1-52a7-43d4-acfb-360cc24c0ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP, IncrementalClassifier\n",
    "from avalanche.training.strategies import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC, CoPE\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.training.strategies import BaseStrategy\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, GEMPlugin, GDumbPlugin\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR10, SplitCIFAR100\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics,ExperienceForgetting\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.strategies import Naive\n",
    "from pl_bolts.models.self_supervised import SwAV\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import SGD\n",
    "from torchvision import transforms\n",
    "from avalanche.training.strategies.icarl import ICaRL\n",
    "import numpy as np\n",
    "from avalanche.benchmarks.classic.ccifar100 import SplitCIFAR100\n",
    "from avalanche.models import IcarlNet, make_icarl_net, initialize_icarl_net\n",
    "from avalanche.training.plugins.lr_scheduling import LRSchedulerPlugin\n",
    "\n",
    "from NeoCL.models.pretrained import PretrainedIncrementalClassifier, SSLIcarl\n",
    "from NeoCL.plugins.sparse_ewc import SparseEWCPlugin\n",
    "from NeoCL.strategies.utils import get_average_metric, create_default_args\n",
    "\n",
    "\n",
    "# create strategy\n",
    "def icarl_cifar100_augment_data(img):\n",
    "    img = img.numpy()\n",
    "    padded = np.pad(img, ((0, 0), (4, 4), (4, 4)), mode='constant')\n",
    "    random_cropped = np.zeros(img.shape, dtype=np.float32)\n",
    "    crop = np.random.randint(0, high=8 + 1, size=(2,))\n",
    "\n",
    "    # Cropping and possible flipping\n",
    "    if np.random.randint(2) > 0:\n",
    "        random_cropped[:, :, :] = \\\n",
    "            padded[:, crop[0]:(crop[0]+32), crop[1]:(crop[1]+32)]\n",
    "    else:\n",
    "        random_cropped[:, :, :] = \\\n",
    "            padded[:, crop[0]:(crop[0]+32), crop[1]:(crop[1]+32)][:, :, ::-1]\n",
    "    t = torch.tensor(random_cropped)\n",
    "    return t\n",
    "fixed_class_order = [87, 0, 52, 58, 44, 91, 68, 97, 51, 15,\n",
    "                            94, 92, 10, 72, 49, 78, 61, 14, 8, 86,\n",
    "                            84, 96, 18, 24, 32, 45, 88, 11, 4, 67,\n",
    "                            69, 66, 77, 47, 79, 93, 29, 50, 57, 83,\n",
    "                            17, 81, 41, 12, 37, 59, 25, 20, 80, 73,\n",
    "                            1, 28, 6, 46, 62, 82, 53, 9, 31, 75,\n",
    "                            38, 63, 33, 74, 27, 22, 36, 3, 16, 21,\n",
    "                            60, 19, 70, 90, 89, 43, 5, 42, 65, 76,\n",
    "                            40, 30, 23, 85, 2, 95, 56, 48, 71, 64,\n",
    "                            98, 13, 99, 7, 34, 55, 54, 26, 35, 39]\n",
    "# config (NOTE: memory_size==k)\n",
    "seed = np.random.randint(1000, 9999)\n",
    "args = create_default_args({'cuda': 0, 'batch_size': 128, 'nb_exp': 10,\n",
    "                            'memory_size': 2000, 'epochs': 70, 'lr_base': 2.,\n",
    "                            'lr_milestones': [49, 63], 'lr_factor': 5.,\n",
    "                            'wght_decay': 0.00001, 'train_mb_size': 256,\n",
    "                            'fixed_class_order': fixed_class_order, 'seed': seed})\n",
    "#\n",
    "device = torch.device(f\"cuda:{args.cuda}\"\n",
    "                      if torch.cuda.is_available() and\n",
    "                         args.cuda >= 0 else \"cpu\")\n",
    "weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar'\n",
    "encoder = SwAV.load_from_checkpoint(weight_path, strict=True)\n",
    "model = SSLIcarl(encoder,embedding_size=2048,num_classes=100).to(device)\n",
    "tb_logger = TensorboardLogger(f'../logs/{args.seed}/')\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    ExperienceForgetting(),\n",
    "    loggers=[tb_logger])\n",
    "benchmark = SplitCIFAR100(n_experiences=args.nb_exp, seed=args.seed,\n",
    "              fixed_class_order=args.fixed_class_order, dataset_root='/share/datasets/')\n",
    "\n",
    "\n",
    "optim = SGD(model.parameters(), lr=args.lr_base,\n",
    "            weight_decay=args.wght_decay, momentum=0.9)\n",
    "sched = LRSchedulerPlugin(\n",
    "    MultiStepLR(optim, args.lr_milestones, gamma=1.0 / args.lr_factor))\n",
    "\n",
    "strategy = ICaRL(\n",
    "    model.feature_extractor, model.classifier, optim,\n",
    "    args.memory_size,\n",
    "    buffer_transform=transforms.Compose([icarl_cifar100_augment_data]),\n",
    "    fixed_memory=True, train_mb_size=args.batch_size,\n",
    "    train_epochs=args.epochs, eval_mb_size=args.batch_size,\n",
    "    plugins=[sched], device=device, evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "# train on the selected scenario with the chosen strategy\n",
    "print('Starting experiment...')\n",
    "dict_iCaRL_aia = {}\n",
    "for i, train_batch_info in enumerate(benchmark.train_stream):\n",
    "    print(\"Start training on experience \", train_batch_info.current_experience)\n",
    "\n",
    "    strategy.train(train_batch_info, num_workers=4)\n",
    "    print(\"End training on experience \", train_batch_info.current_experience)\n",
    "    print('Computing accuracy on the test set')\n",
    "    res = strategy.eval(benchmark.test_stream[:i + 1], num_workers=4)\n",
    "    dict_iCaRL_aia['Top1_Acc_Stream/Exp'+str(i)] = res['Top1_Acc_Stream/eval_phase/test_stream/Task000']\n",
    "    avg_ia = get_average_metric(dict_iCaRL_aia)\n",
    "    print(\"dict_iCaRL_aia= \", dict_iCaRL_aia)\n",
    "    print(f\"scifar100-batch=10 Average Incremental Accuracy: {avg_ia:.5f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452945-47b4-432b-ac1d-ff61e6c556ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dev-2021-02-py38': venv)",
   "language": "python",
   "name": "python38564bitdev202102py38venved5311bfb7744d6caaf9ece0ad44c9d0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
